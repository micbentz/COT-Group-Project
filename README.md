# COT-Group-Project

PROJECT DESIDERATA:
Each team needs to turn in ONE pdf of the final project. This has to be emailed to Anand Rangarajan at anand@cise.ufl.edu with the subject title Final Project: COT4501: Spring 2015 and attachment entitled cot4501sp15_project_final_<insert team member last names separated by _ >.pdf. Please put each team member's name and UFID in the submission.  The final project is due Saturday, May 2nd by midnight. If the file is too big and/or if you'd like to show us demonstrations of your code, please place the entire project on google drive, dropbox, skydrive or spideroak and send a direct link. Ask for confirmation from me that I received and downloaded your project.

The actual assignment: 
Download the Iris and Wine datasets available at the UCI Machine Learning repository in the platform of your choice. Describe the datasets in your final project submission.
Download the libsvm library (for support vector machines - SVM). This is the competing industry strength classifier that y'all will be executing. Write up a description of how this library was executed by your team on the Wine and Iris datasets (and any other datasets you used with further details provided under the extra credit section below).  Demonstrate the execution of the SVM classifier on the datasets by first training the classifier on a suitably chosen training set and then testing the classifier on a separate test set (where you know the class labels for each pattern but the machine does not).
Execute a homegrown least-squares classifier on the same datasets with identical choices of training and test set patterns as the SVM. Document the mis-classification errors (and separate out each class errors as well) for both the SVM and the least-squares classifiers. Document the settings of the free parameters in both the SVM and the least-squares classifiers for each dataset. Give a high level summary of your findings based on your interpretation of the results.
For each dataset, the team should first separate out training and testing patterns. The ratios attempted for training and testing should range from 10% training/90% testing to 50% training/50% testing. For the sake of cross validation, training sets can be further subdivided into training and validation sets with more details provided in the libsvm practical guide.

Extra credit: 
To get maximum extra credit (up to an additional 10% for the project grade), attempt two more datasets including the student generated social networks dataset if it becomes available. Repeat as above and clearly separate this work from the rest. Explain your choices of the additional datasets and document the free parameters, the errors and report your findings as above.

